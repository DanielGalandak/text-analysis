# services/gpt_chunking_from_sentences.py

from utils.gpt_client import client
from repositories.load_prompt import load_prompt
import json

MAX_SENTENCES_PER_BATCH = 20  # bezpeÄnÃ½ limit pro GPT-4o, klidnÄ› pozdÄ›ji doladÃ­me

def group_sentences_into_chunks(sentence_blocks, prompt_name="2025_04_23_chunking_prompt.txt"):
    """
    RozdÄ›lÃ­ vÄ›ty na dÃ¡vky a posÃ­lÃ¡ je postupnÄ› do GPT.
    """
    prompt_template = load_prompt(prompt_name)

    # PÅ™ipravÃ­me vÅ¡echny vÄ›ty
    all_sentences = []
    for block in sentence_blocks:
        for s in block["sentences"]:
            all_sentences.append({
                "id": s["id"],
                "annotation": s["annotation"]
            })

    total_sentences = len(all_sentences)
    print(f"ğŸ“„ Celkem vÄ›t k chunkovÃ¡nÃ­: {total_sentences}")

    chunks = []
    batch_number = 0

    # RozdÄ›lÃ­me vÄ›ty na dÃ¡vky
    for i in range(0, total_sentences, MAX_SENTENCES_PER_BATCH):
        batch_sentences = all_sentences[i:i+MAX_SENTENCES_PER_BATCH]
        batch_number += 1
        print(f"âœ‰ï¸ OdesÃ­lÃ¡m batch {batch_number}: {len(batch_sentences)} vÄ›t")

        # PÅ™ipravÃ­me prompt pro tuto batch
        prompt = prompt_template.format(sentences=json.dumps(batch_sentences, ensure_ascii=False, indent=2))

        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "Jsi specialista na vÃ½roÄnÃ­ zprÃ¡vy. OdpovÃ­dej pouze ÄistÃ½m JSONem podle specifikace."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.2
        )

        content = response.choices[0].message.content.strip()
        
        if content.startswith("```json"):
            content = content.replace("```json", "").replace("```", "").strip()

        try:
            result = json.loads(content)

            if isinstance(result, dict) and "chunks" in result:
                chunks.extend(result["chunks"])
            elif isinstance(result, list):
                chunks.extend(result)  # pÅ™Ã­mo extendujeme list chunkÅ¯
            else:
                raise ValueError("âš ï¸ NeoÄekÃ¡vanÃ½ formÃ¡t odpovÄ›di od GPT.")

            print(f"âœ… Batch {batch_number} zpracovÃ¡n.")

        except Exception as e:
            print(f"âŒ Chyba pÅ™i zpracovÃ¡nÃ­ batch {batch_number}: {str(e)}")
            print(f"âš ï¸ ObdrÅ¾enÃ½ obsah:\n{content}")
            raise e

    print(f"âœ… Celkem vytvoÅ™eno {len(chunks)} tematickÃ½ch chunkÅ¯.")
    return chunks    

def assemble_paragraphs_from_chunks(sentence_annotations, chunk_structure):
    """
    VytvoÅ™Ã­ seznam paragraphÅ¯ {paragraph_id, text} na zÃ¡kladÄ› pÅ¯vodnÃ­ch anotacÃ­ vÄ›t a struktury chunkÅ¯.

    :param sentence_annotations: list pÅ¯vodnÃ­ch vÄ›tnÃ½ch blokÅ¯ (z annotated_sentences.json)
    :param chunk_structure: list chunkÅ¯ od GPT (chunk_id + sentence_ids)
    :return: list paragraphÅ¯ {paragraph_id, text}
    """
    # 1. Mapa ID -> text vÄ›ty
    id_to_sentence = {s["id"]: s["text"] for block in sentence_annotations for s in block["sentences"]}

    # 2. StavÃ­me seznam paragraphÅ¯
    paragraphs = []
    for chunk in chunk_structure:
        paragraph_id = chunk["chunk_id"]
        sentence_texts = [id_to_sentence[sid] for sid in chunk["sentence_ids"] if sid in id_to_sentence]
        paragraph_text = " ".join(sentence_texts)
        paragraphs.append({
            "paragraph_id": paragraph_id,
            "text": paragraph_text
        })

    return paragraphs

